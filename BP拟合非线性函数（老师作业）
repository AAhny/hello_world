clc          %清除命令窗口的内容，对工作环境中的全部变量无任何影响 
clear        %清除工作空间的所有变量 
close all    %关闭所有的Figure窗口
x=-1:0.01:1; %生成训练数据x     数组X[-1,-0.09,-0.08.......0,0.01,0.02,.....1]  长度200
y=-1:0.01:1; %                 数组Y[-1,-0.09,-0.08.......0,0.01,0.02,.....1]  长度200
% 神经网络层数定义 Init_BP() 
Input=2;              %输入层向量2维
HideLayer=5;          %隐含层向量5维
Output=1;             %输出层向量1维
%权值及阈值初始化
w1=rands(HideLayer,Input);  %隐含层的权值   rands(列，行)
b1=rands(HideLayer,1);      %隐含层的阈值
w2=rands(Output,HideLayer); %输出层的权值
b2=rands(Output,1);         %输出层的阈值
error = 0.01;%误差阈值
learnrate=0.2; %学习速率
[X,Y]=meshgrid(x,y); 
M=201;
for m1=1:M    %外循环 200次
   for m2=1:M     %内循环200次
      z=sin(2.*pi*x(m1))*sin(2.*pi*y(m2));   %调用数组x，y的值相*  计算神经网络应该得到的值
      for k=1:100
        %  正向传播
         for i=1:HideLayer   %隐层的输出
             %Hout为一个数组  其中每个数代表 隐含层一个结点的输出   %logsig()表示神经网络S函数求和（Xi*Wij）
             HOut(i)=logsig(x(m1)*w1(i,1)+y(m2)*w1(i,2)+b1(i,1));      
         end
         %隐藏层的输出层
         a=0;%输出层
         for j=1:HideLayer%求解输出层的输入
             Out_In(j)=HOut(j)*w2(1,j);
             a=a+Out_In(j);
         end
         a=a+b2;
         if z>=0 %输出层的输出              
             Out_Out = logsig(a);
         elseif z<0
             Out_Out = -logsig(a);
         end
        %误差
         e0 = z - Out_Out;
         e = e0^2/2; 
        %反向传播
        Delta_Out = -e0 * (1 - Out_Out) * Out_Out;   %反向误差信号   OK*（1-OK）*（dk-OK）
        for k = 1:HideLayer
             Delta_HideLayer(k) = Delta_Out * w2(1,k) * (1 - HOut(k)) * HOut(k);    %% 输出层的权值调整量   %调整量   求和 [学习率*(反向误差信号*W2)] * yi(1-yi)
        end
        for k=1:HideLayer
              w2(1,k)=w2(1,k)-learnrate*Delta_Out*HOut(k);   %% 输出层误差调整量  %反向误差传递信号*学习率*yi    
         end
         b2(1,1)=b2(1,1)-learnrate*Delta_Out;               %% 输出层阈值调整
        % 隐含层权值调整
         for i=1:HideLayer
               for j=1:Input
                        if j==1
                            w1(i,j)=learnrate*Delta_HideLayer(i)*x(m1);  
                        else
                            w1(i,j)=learnrate*Delta_HideLayer(i)*y(m2);
                        end
               end
               b1(i,1)=b1(i,1)+learnrate*Delta_HideLayer(i);  %隐含层阈值调整
         end
         if e < error
                break;
         end
      end
      z1(m2,m1) = Out_Out;  %记录当前的神经网络输出值
    end
end
% Draw_function(),绘制神经网络逼近的图像
figure;
mesh(X,Y,z1);
title('BP神经网络逼近sin(2.*pi*x)*sin(2.*pi*y)图像');
hold on;
save data_w1.txt w1 -ascii
save data_w2.txt w2 -ascii
save data_b1.txt b1 -ascii
save data_b2.txt b2 -ascii




